# Newscope example configuration
# Copy this file to config.toml and edit values as needed.
#
# Notes:
# - Multiple users may be defined here (for MVP users are configured via this file).
# - Secrets (API keys) for remote LLMs should be provided via environment variables when possible.
# - Times use HH:MM 24-hour format and are interpreted in the host's local timezone.
# - Paths may be relative to the working directory or absolute.

# -------------------------
# Database configuration
# -------------------------
[database]
# Path to the SQLite database file. The directory will be created if missing.
path = "data/newscope.db"

# -------------------------
# Server / runtime options
# -------------------------
[server]
# Address and port to bind the HTTP server (Rocket will listen here).
bind = "0.0.0.0"
port = 8000

# If true, the server will run in read-only mode for the UI (no write operations).
# Useful for demo or debugging scenarios. Default: false
read_only = false

# -------------------------
# Scheduler (worker) config
# -------------------------
[scheduler]
# Times at which the ingestion worker should run. Default example: 05:00, 11:00, 17:00, 23:00
times = ["05:00", "11:00", "17:00", "23:00"]

# Maximum number of concurrent feed fetch tasks overall (not per-domain).
max_concurrent_fetches = 8

# -------------------------
# Fetch / politeness settings
# -------------------------
[politeness]
# Delay in seconds between requests to the same domain (default: 1 second)
delay_seconds = 1

# Max concurrent requests allowed per domain (default: 2)
concurrency_per_domain = 2

# Maximum response size in bytes to fetch and process (default 524288 = 512 KiB)
max_response_bytes = 524288

# HTTP timeout in seconds for fetch operations
fetch_timeout_seconds = 10

# Respect robots.txt? Default: false (user-controlled personal app).
# Set to true to honor robots rules for crawlers.
respect_robots_txt = false

# -------------------------
# LLM / AI configuration
# -------------------------
[llm]
# Adapter can be "local", "remote", or "none"
# - "local" expects a configured local model path/engine
# - "remote" will call an external API endpoint
# - "none" disables LLM features (extractive fallback only)
adapter = "remote"

# Local model configuration (only used when adapter = "local")
[llm.local]
# Path to local model or engine config (example placeholder)
model_path = "/opt/models/ggml-model.bin"
# Maximum threads for local inference (set to a low number on RPi)
max_threads = 1

# Remote provider configuration (only used when adapter = "remote")
[llm.remote]
# URL of the remote LLM API (Ollama default)
api_url = "http://localhost:11434/v1/chat/completions"

# Name of the environment variable that contains the API key
# For Ollama, this can be any value, but the env var must exist.
api_key_env = "OLLAMA_API_KEY"

# Model to use (Ollama model name)
model = "llama3:latest"

# Timeout in seconds for remote LLM calls
timeout_seconds = 60

# Maximum tokens to generate
max_tokens = 500

# NEW: Background processing LLM (powerful, slow OK)
# Used for article summarization during scheduled ingestion
[llm.background]
api_url = "http://localhost:11434/v1/chat/completions"
api_key_env = "OLLAMA_API_KEY"
model = "llama3:latest"  # or "mistral:latest" for better quality
timeout_seconds = 3600   # 1 hour max per article (very generous)
max_tokens = 1000        # More tokens for detailed summaries

# NEW: Interactive LLM (fast, lightweight)
# Used for press review generation and chat
[llm.interactive]
api_url = "http://localhost:11434/v1/chat/completions"
api_key_env = "OLLAMA_API_KEY"
model = "tinyllama:latest"  # or "phi3:latest" for better quality
timeout_seconds = 60     # 1 minute max
max_tokens = 500         # Shorter responses for speed

# -------------------------
# Logging
# -------------------------
[logging]
# Default log level: "info", "debug", "warn", "error"
level = "info"

# -------------------------
# Retention and storage options
# -------------------------
[storage]
# Number of days to keep archived sessions and chat transcripts.
# Set to 0 to keep indefinitely.
session_retention_days = 0

# -------------------------
# Feed import and OPML
# -------------------------
[import]
# Default directory to store imported OPML files (optional)
opml_import_dir = "data/opml_imports"

# When importing many feeds in parallel, use bounded concurrency to avoid overload
# during import operations.
import_concurrency = 6

# -------------------------
# Scoring defaults (tunable)
# -------------------------
[scoring]
# Weights used for computing article scores. Tunable parameters:
# final_score = w_pref*preference + w_red*redundancy + w_recency*recency + w_src*source_weight + w_novel*novelty
w_pref = 1.5
w_red = 2.0
w_recency = 1.0
w_src = 0.5
w_novel = 0.75

# Serendipity factor: fraction [0.0 - 1.0] controlling occasional surfacing of novel items
serendipity = 0.05

# -------------------------
# Admin / maintenance
# -------------------------
[admin]
# If true, the application will automatically run DB migrations at startup.
auto_migrate = true

# Path to a directory where the app can write diagnostics or exports
diagnostics_dir = "data/diagnostics"

# -------------------------
# Examples of environment usage (documentational)
# -------------------------
# To run with a remote LLM API key, export the environment variable named above:
#   export NEWSCOPE_LLM_API_KEY=\"your_api_key_here\"
#
# Example command lines:
#   ./newscope --config config.toml                 # start server + worker (default)
#   ./newscope --config config.toml --no-worker    # start server only
#   ./newscope --config config.toml --worker-only  # run worker only (no HTTP)
#
# End of example config.
